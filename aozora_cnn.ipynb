{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is data integrating and training script for author classification using Keras with character-level cnn.\n",
    "Be sure to get the text files from Aozora-bunko and convert them to UTF-8 encoded csv files, just by running aozora-scrape.py.\n",
    "\"\"\"\n",
    "\n",
    "import sys, os.path, re, csv, os, glob\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import codecs\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Convolution2D, MaxPooling2D, Reshape, Input, merge\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, Callback, CSVLogger, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Setting work directory and opening csv files.\n",
    "base_url = \"http://www.aozora.gr.jp/\"\n",
    "data_dir = \"./\"\n",
    "aozora_dir = data_dir + \"aozora_data/\"\n",
    "log_dir = aozora_dir + \"log/\"\n",
    "target_author_file = data_dir + \"target_author.csv\"\n",
    "\n",
    "auth_target = []\n",
    "with open(target_author_file,\"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        auth_target.append(row)\n",
    "auth_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating integrated file of csv's of all the pieces for each author.\n",
    "# One csv file is generated and saved for each author.\n",
    "\n",
    "def author_data_integ(auth_target=auth_target):\n",
    "    for w in auth_target[1:]:\n",
    "        print (\"starting: \" + w[0])\n",
    "        auth_dir = '{}{}/'.format(aozora_dir, w[0])\n",
    "        csv_dir = '{}{}'.format(auth_dir, \"csv/\")\n",
    "        files = os.listdir(csv_dir)\n",
    "        integ_np = np.array([[\"author\", \"line\"]])\n",
    "        for file in files:\n",
    "            if \"csv\" in file:\n",
    "                print (\"   now at: \" + file)\n",
    "                file_name = csv_dir + file\n",
    "                pds_data = pds.read_csv(file_name, index_col=0)\n",
    "                pds_data = pds_data.dropna()\n",
    "                np_data = np.array(pds_data.ix[:,[0,2]])\n",
    "\n",
    "                out = [j for j in range(len(np_data)) if '-----------' in str(np_data[j,1])]\n",
    "                if not out: out = [1]\n",
    "                hyphen_pos = int(out[len(out) - 1])\n",
    "\n",
    "                last_20 = len(np_data) - 20\n",
    "\n",
    "                np_data = np_data[hyphen_pos+1:last_20,:]\n",
    "                integ_np = np.vstack((integ_np, np_data))\n",
    "\n",
    "        integ_pds = pds.DataFrame(integ_np[1:,:], columns=integ_np[0,:])\n",
    "        integ_pds.to_csv(auth_dir + w[0] + '_integ.csv', quoting=csv.QUOTE_ALL)\n",
    "        print (\"finished: \" + w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_data_integ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading integrated csv files.\n",
    "\n",
    "def load_integ(author, auth_dir):\n",
    "    integ_csv = auth_dir + author + \"_integ.csv\"\n",
    "    data = pds.read_csv(integ_csv)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "natsume_data = load_integ(auth_target[1][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[1][0]))\n",
    "np_natsume = np.array(natsume_data.ix[1:,1:])\n",
    "\n",
    "akutagawa_data = load_integ(auth_target[2][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[2][0]))\n",
    "np_akutagawa = np.array(akutagawa_data.ix[1:,1:])\n",
    "\n",
    "mori_data = load_integ(auth_target[3][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[3][0]))\n",
    "np_mori = np.array(mori_data.ix[1:,1:])\n",
    "\n",
    "sakaguchi_data = load_integ(auth_target[4][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[4][0]))\n",
    "np_sakaguchi = np.array(sakaguchi_data.ix[1:,1:])\n",
    "\n",
    "yoshikawa_data = load_integ(auth_target[5][0], auth_dir = '{}{}/'.format(aozora_dir, auth_target[5][0]))\n",
    "np_yoshikawa = np.array(yoshikawa_data.ix[1:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing arrays\n",
    "\n",
    "natsume_txt = np.array([np_natsume[:,1]]).T\n",
    "akutagawa_txt = np.array([np_akutagawa[:,1]]).T\n",
    "mori_txt = np.array([np_mori[:,1]]).T\n",
    "sakaguchi_txt = np.array([np_sakaguchi[:,1]]).T\n",
    "yoshikawa_txt = np.array([np_yoshikawa[:,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "natsume_id = np.array([np.zeros(len(np_natsume))]).T\n",
    "akutagawa_id = np.array([np.zeros(len(np_akutagawa)) + 1]).T\n",
    "mori_id = np.array([np.zeros(len(np_mori)) + 2]).T\n",
    "sakaguchi_id = np.array([np.zeros(len(np_sakaguchi)) + 3]).T\n",
    "yoshikawa_id = np.array([np.zeros(len(np_yoshikawa)) + 4]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "natsume = np.hstack((natsume_txt, natsume_id))\n",
    "akutagawa = np.hstack((akutagawa_txt, akutagawa_id))\n",
    "mori = np.hstack((mori_txt, mori_id))\n",
    "sakaguchi = np.hstack((sakaguchi_txt, sakaguchi_id))\n",
    "yoshikawa = np.hstack((yoshikawa_txt, yoshikawa_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (len(natsume), len(akutagawa), len(mori), len(sakaguchi), len(yoshikawa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This time, I am using pieces from Soseki Natsume, Ryunosuke Akutagawa, Ogai Mori and Ango Sakaguchi\n",
    "for they have about the same number of lines.\n",
    "Omitting Eiji Yoshikawa due to the fact that he has three time more line of texts than others.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_integ = np.vstack((np.vstack((np.vstack((natsume, akutagawa)),mori)),sakaguchi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting each line of texts to encoded array.\n",
    "\n",
    "def load_data(txt, max_length=200):\n",
    "    txt_list = []\n",
    "    for l in txt:\n",
    "        txt_line = [ord(x) for x in str(l).strip()]\n",
    "        # You will get encoded text in array, just like this\n",
    "        # [25991, 31456, 12391, 12399, 12394, 12367, 12387, 12390, 23383, 24341, 12391, 12354, 12427, 12290]\n",
    "        txt_line = txt_line[:max_length]\n",
    "        txt_len = len(txt_line)\n",
    "        if txt_len < max_length:\n",
    "            txt_line += ([0] * (max_length - txt_len))\n",
    "        txt_list.append((txt_line))\n",
    "    return txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making arrays for training text and target author\n",
    "txt_list = load_data(txt = data_integ[:,0])\n",
    "np_txt = np.array(txt_list)\n",
    "\n",
    "tgt_list = data_integ[:,1]\n",
    "np_tgt_list = np_utils.to_categorical(tgt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating character-level convolutional neural network model.\n",
    "\n",
    "def create_model(embed_size=128, max_length=200, filter_sizes=(2, 3, 4, 5), filter_num=64):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    emb = Embedding(0xffff, embed_size)(inp)\n",
    "    emb_ex = Reshape((max_length, embed_size, 1))(emb)\n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Convolution2D(filter_num, filter_size, embed_size, activation=\"relu\")(emb_ex)\n",
    "        pool = MaxPooling2D(pool_size=(max_length - filter_size + 1, 1))(conv)\n",
    "        convs.append(pool)\n",
    "    convs_merged = merge(convs, mode='concat')\n",
    "    reshape = Reshape((filter_num * len(filter_sizes),))(convs_merged)\n",
    "    fc1 = Dense(64, activation=\"relu\")(reshape)\n",
    "    bn1 = BatchNormalization()(fc1)\n",
    "    do1 = Dropout(0.5)(bn1)\n",
    "    fc2 = Dense(4, activation='sigmoid')(do1)\n",
    "    model = Model(input=inp, output=fc2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the model.\n",
    "\n",
    "def train(inputs, targets, batch_size=100, epoch_count=100, max_length=200, model_filepath=aozora_dir + \"model.h5\", learning_rate=0.001):\n",
    "    start = learning_rate\n",
    "    stop = learning_rate * 0.01\n",
    "    learning_rates = np.linspace(start, stop, epoch_count)\n",
    "\n",
    "    model = create_model(max_length=max_length)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    target = os.path.join('/tmp', 'weights.*.hdf5')\n",
    "    files = [(f, os.path.getmtime(f)) for f in glob.glob(target)]\n",
    "    if len(files) != 0:\n",
    "        latest_saved_model = sorted(files, key=lambda files: files[1])[-1]\n",
    "        model.load_weights(latest_saved_model[0])\n",
    "    \n",
    "    # Logging file for each epoch\n",
    "    csv_logger_file = '/tmp/clcnn_training.log'\n",
    "    \n",
    "    # Checkpoint model for each epoch\n",
    "    checkpoint_filepath = \"/tmp/weights.{epoch:02d}-{loss:.2f}-{acc:.2f}-{val_loss:.2f}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "    model.fit(inputs, targets,\n",
    "              nb_epoch=epoch_count,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              validation_split=0.1,\n",
    "              shuffle=True,\n",
    "              callbacks=[\n",
    "                  LearningRateScheduler(lambda epoch: learning_rates[epoch]),\n",
    "                  CSVLogger(csv_logger_file),\n",
    "                  ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, save_best_only=True, save_weights_only=False, monitor='val_acc')\n",
    "              ])\n",
    "\n",
    "    model.save(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(np_txt, np_tgt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
